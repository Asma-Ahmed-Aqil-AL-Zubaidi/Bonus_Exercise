{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asma-Ahmed-Aqil-AL-Zubaidi/Bonus_Exercise/blob/main/Huggingface_Tokenizer_Exercise(Bonus).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec47c97d",
      "metadata": {
        "id": "ec47c97d"
      },
      "source": [
        "# Exercise: Working with Hugging Face Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7668610a",
      "metadata": {
        "id": "7668610a"
      },
      "source": [
        "### Part 1: Basic Tokenization with Hugging Face Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd04b75",
      "metadata": {
        "id": "5fd04b75"
      },
      "source": [
        "\n",
        "1. Install the Hugging Face `transformers` library:\n",
        "   ```bash\n",
        "   !pip install transformers\n",
        "   ```\n",
        "\n",
        "2. Choose a pre-trained tokenizer from Hugging Faceâ€™s model hub (e.g., `bert-base-uncased`, `gpt2`, etc.) and tokenize a piece of text:\n",
        "   \n",
        "   **Task**: Load the tokenizer and tokenize the sentence: `\"T5 is the greatest data science boot-camp!\"`\n",
        "\n",
        "   Below is a code block where you can perform this task:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "rvp6ss9CTjLD",
        "outputId": "4fa033c5-7093-44d6-b995-0357121c6aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rvp6ss9CTjLD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dffd763",
      "metadata": {
        "id": "9dffd763",
        "outputId": "3c8bb0f5-8cd8-45ab-dbfd-fd459eccbf55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "ilZLv3ZRTxL7"
      },
      "id": "ilZLv3ZRTxL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
      ],
      "metadata": {
        "id": "96KBNy-UXRur"
      },
      "id": "96KBNy-UXRur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1292c1",
      "metadata": {
        "id": "9e1292c1",
        "outputId": "a6d82579-c47f-447a-db92-eb7d1c0aeb30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['t', '##5', 'is', 'the', 'greatest', 'data', 'science', 'boot', '-', 'camp', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"T5 is the greatest data science boot-camp!\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27c9634",
      "metadata": {
        "id": "e27c9634"
      },
      "source": [
        "\n",
        "### Part 2: Encoding and Decoding\n",
        "\n",
        "3. Use the same tokenizer to encode the sentence (convert to token IDs) and then decode it back to text.\n",
        "\n",
        "   **Task**: Encode the sentence and then decode it back to text.\n",
        "\n",
        "   Below is a code block where you can perform this task:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer.encode(text, add_special_tokens=True)\n",
        "print(f\"Encoded token IDs: {encoded_input}\")\n",
        "decoded_text = tokenizer.decode(encoded_input)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "id": "Hz6OY8vaW8W9",
        "outputId": "79a04896-56b9-4fd2-8960-71c778e96823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Hz6OY8vaW8W9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded token IDs: [101, 1056, 2629, 2003, 1996, 4602, 2951, 2671, 9573, 1011, 3409, 999, 102]\n",
            "Decoded text: [CLS] t5 is the greatest data science boot - camp! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a8df7c",
      "metadata": {
        "id": "14a8df7c"
      },
      "source": [
        "\n",
        "### Bonus Challenge\n",
        "\n",
        "4. **Custom Tokenizer**: Use Hugging Faceâ€™s `tokenizers` library to train a custom tokenizer on a dataset.\n",
        "   \n",
        "   You are provided with a dataset containing multiple sentences. Train a custom tokenizer using these sentences.\n",
        "\n",
        "   Below is a code block to train the tokenizer:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b10db58",
      "metadata": {
        "id": "6b10db58",
        "outputId": "2d498071-81a1-47d4-bdf8-0d34d5abb936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Transformers are amazing for NLP tasks.', 'Tokenization is essential for language models.', 'Byte Pair Encoding is a great subword tokenization algorithm.', 'Hugging Face makes it easy to work with pre-trained models.', 'Data science is the key to unlocking insights from data.']\n"
          ]
        }
      ],
      "source": [
        "dataset = [\n",
        "    \"Transformers are amazing for NLP tasks.\",\n",
        "    \"Tokenization is essential for language models.\",\n",
        "    \"Byte Pair Encoding is a great subword tokenization algorithm.\",\n",
        "    \"Hugging Face makes it easy to work with pre-trained models.\",\n",
        "    \"Data science is the key to unlocking insights from data.\"\n",
        "]\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train_from_iterator(dataset, trainer)"
      ],
      "metadata": {
        "id": "JJe_FY3fXOE9"
      },
      "id": "JJe_FY3fXOE9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"custom_tokenizer.json\")"
      ],
      "metadata": {
        "id": "RJoyiiskYPaf"
      },
      "id": "RJoyiiskYPaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"This is a test for the custom tokenizer.\")"
      ],
      "metadata": {
        "id": "euMBSZndYPl3"
      },
      "id": "euMBSZndYPl3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokens: {output.tokens}\")\n",
        "print(f\"Token IDs: {output.ids}\")"
      ],
      "metadata": {
        "id": "Zv6IgZc9YctE",
        "outputId": "0060388f-4bdf-49dd-c7b0-cbc30efcde8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Zv6IgZc9YctE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['T', '##h', '##i', '##s', 'is', 'a', 't', '##e', '##s', '##t', 'for', 'the', 'c', '##u', '##s', '##t', '##om', 'to', '##ken', '##iz', '##er', '.']\n",
            "Token IDs: [15, 59, 42, 47, 66, 16, 33, 46, 47, 50, 70, 173, 18, 49, 47, 50, 123, 67, 80, 76, 136, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### :)"
      ],
      "metadata": {
        "id": "mLv7eXXaYn5k"
      },
      "id": "mLv7eXXaYn5k"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}